{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oj7EEGIt6bU4",
        "outputId": "6bdb2aec-452a-4a38-8c6d-fe84f6e617bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faster-whisper\n",
            "  Downloading faster_whisper-1.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting ctranslate2<5,>=4.0 (from faster-whisper)\n",
            "  Downloading ctranslate2-4.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (0.33.0)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (0.21.1)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting av>=11 (from faster-whisper)\n",
            "  Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (2.0.2)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (1.1.4)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2025.6.15)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
            "Downloading faster_whisper-1.1.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, ctranslate2, av, coloredlogs, onnxruntime, faster-whisper\n",
            "Successfully installed av-14.4.0 coloredlogs-15.0.1 ctranslate2-4.6.0 faster-whisper-1.1.1 humanfriendly-10.0 onnxruntime-1.22.0\n",
            "Collecting noisereduce\n",
            "  Downloading noisereduce-3.0.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from noisereduce) (1.15.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from noisereduce) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from noisereduce) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from noisereduce) (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from noisereduce) (1.5.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.17.0)\n",
            "Downloading noisereduce-3.0.3-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: noisereduce\n",
            "Successfully installed noisereduce-3.0.3\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from soundfile) (2.0.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.6.15)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,059 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,572 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,561 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,733 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,036 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,349 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,750 kB]\n",
            "Fetched 29.5 MB in 4s (7,447 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "35 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install faster-whisper\n",
        "!pip install noisereduce\n",
        "!pip install soundfile librosa\n",
        "!pip install pydub\n",
        "!pip install scipy\n",
        "\n",
        "# Install ffmpeg for audio processing\n",
        "!apt update && apt install -y ffmpeg\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import noisereduce as nr\n",
        "from faster_whisper import WhisperModel\n",
        "import time\n",
        "from pydub import AudioSegment\n",
        "import requests\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "zHEfTF2H6_p2",
        "outputId": "c5908b9c-1d20-45b7-ba8a-994e78f0b4d4"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 162)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m162\u001b[0m\n\u001b[0;31m    for file in files:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "### Use LibriMix Dataset (Mini Version for Testing)\n",
        "\n",
        "def setup_librimix_mini():\n",
        "    \"\"\"\n",
        "    Set up a mini version of LibriMix for testing in Google Colab\n",
        "    Full LibriMix is 430GB - we'll create a smaller subset\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Setting up LibriMix mini dataset for testing...\")\n",
        "\n",
        "    # Install SoX (required for LibriMix)\n",
        "    print(\"Installing SoX...\")\n",
        "    !apt-get update -qq\n",
        "    !apt-get install -y -qq sox\n",
        "\n",
        "    # Clone LibriMix repository\n",
        "    print(\"\\nCloning LibriMix repository...\")\n",
        "    !git clone https://github.com/JorisCos/LibriMix\n",
        "\n",
        "    # Create storage directory\n",
        "    os.makedirs('librimix_data', exist_ok=True)\n",
        "\n",
        "    # Generate a minimal subset\n",
        "    generate_minimal_librimix()\n",
        "\n",
        "\n",
        "def generate_minimal_librimix():\n",
        "    \"\"\"\n",
        "    Generate a minimal LibriMix dataset suitable for Colab\n",
        "    ~100 samples instead of full dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # Modify the generation script for minimal data\n",
        "    script_content = \"\"\"#!/bin/bash\n",
        "\n",
        "    # Minimal LibriMix generation for testing\n",
        "    # Only generates 2-speaker mixtures at 16kHz\n",
        "\n",
        "    storage_dir=$1\n",
        "    n_src=2\n",
        "\n",
        "    # Download minimal LibriSpeech subset\n",
        "    echo \"Downloading LibriSpeech test-clean...\"\n",
        "    mkdir -p $storage_dir/LibriSpeech\n",
        "    cd $storage_dir/LibriSpeech\n",
        "    wget -q --show-progress http://www.openslr.org/resources/12/test-clean.tar.gz\n",
        "    tar -xzf test-clean.tar.gz\n",
        "    cd ../..\n",
        "\n",
        "    # Download minimal WHAM noise\n",
        "    echo \"Downloading WHAM noise samples...\"\n",
        "    mkdir -p $storage_dir/wham_noise\n",
        "    # Note: Full WHAM is large. For testing, we'll use ESC-50 as substitute\n",
        "    cd $storage_dir/wham_noise\n",
        "    wget -q --show-progress https://github.com/karoldvl/ESC-50/archive/master.zip\n",
        "    unzip -q master.zip\n",
        "    mv ESC-50-master/* .\n",
        "    rm -rf ESC-50-master master.zip\n",
        "    cd ../..\n",
        "\n",
        "    # Generate metadata for mini version\n",
        "    cd LibriMix\n",
        "    python scripts/create_librimix_from_metadata.py \\\n",
        "        --librispeech_dir $storage_dir/LibriSpeech \\\n",
        "        --wham_dir $storage_dir/wham_noise \\\n",
        "        --metadata_dir metadata/Libri2Mix \\\n",
        "        --librimix_outdir $storage_dir/Libri2Mix_mini \\\n",
        "        --n_src $n_src \\\n",
        "        --freqs 16k \\\n",
        "        --modes min \\\n",
        "        --types mix_clean mix_both mix_single \\\n",
        "        --max_samples 35  # Limit samples per type\n",
        "    \"\"\"\n",
        "\n",
        "    with open('generate_librimix_mini.sh', 'w') as f:\n",
        "        f.write(script_content)\n",
        "\n",
        "    !chmod +x generate_librimix_mini.sh\n",
        "    !./generate_librimix_mini.sh librimix_data\n",
        "\n",
        "    # Process the generated files for our 10-second requirement\n",
        "    process_librimix_to_10s()\n",
        "\n",
        "def process_librimix_to_10s():\n",
        "    \"\"\"\n",
        "    Process LibriMix samples to ensure they are exactly 10 seconds\n",
        "    \"\"\"\n",
        "    import soundfile as sf\n",
        "\n",
        "    print(\"\\nProcessing LibriMix samples to 10-second clips...\")\n",
        "\n",
        "    os.makedirs('sample_audios', exist_ok=True)\n",
        "\n",
        "    # Define mixture types\n",
        "    mixture_types = ['mix_clean', 'mix_both', 'mix_single']\n",
        "\n",
        "    for mix_type in mixture_types:\n",
        "        print(f\"\\nProcessing {mix_type}...\")\n",
        "\n",
        "        # Create subdirectories\n",
        "        os.makedirs(f'sample_audios/{mix_type}', exist_ok=True)\n",
        "\n",
        "        # Find generated files\n",
        "        mix_dir = f'librimix_data/Libri2Mix_mini/wav16k/min/dev/{mix_type}'\n",
        "\n",
        "        if os.path.exists(mix_dir):\n",
        "            files = [f for f in os.listdir(mix_dir) if f.endswith('.wav')]\n",
        "\n",
        "            # Process up to 35 samples per type\n",
        "            for i, filename in enumerate(files[:35]):\n",
        "                if 'mix' in filename:\n",
        "                    # Load audio\n",
        "                    audio_path = os.path.join(mix_dir, filename)\n",
        "                    audio, sr = sf.read(audio_path)\n",
        "\n",
        "                    # Ensure 10 seconds (160000 samples at 16kHz)\n",
        "                    target_samples = 10 * 16000\n",
        "\n",
        "                    if len(audio) > target_samples:\n",
        "                        audio = audio[:target_samples]\n",
        "                    else:\n",
        "                        # Pad with zeros\n",
        "                        padding = target_samples - len(audio)\n",
        "                        audio = np.pad(audio, (0, padding), mode='constant')\n",
        "\n",
        "                    # Save processed audio\n",
        "                    output_path = f'sample_audios/{mix_type}/sample_{i:03d}.wav'\n",
        "                    sf.write(output_path, audio, sr)\n",
        "\n",
        "            print(f\"✓ Processed {len(files[:35])} {mix_type} samples\")\n",
        "\n",
        "def create_simple_librimix_alternative():\n",
        "    \"\"\"\n",
        "    Alternative: Create LibriMix-style dataset without full generation\n",
        "    This is faster and more suitable for Colab\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Creating LibriMix-style dataset (simplified version)...\")\n",
        "\n",
        "    os.makedirs('sample_audios', exist_ok=True)\n",
        "\n",
        "    # Download LibriSpeech test-clean\n",
        "    print(\"Downloading LibriSpeech test-clean...\")\n",
        "    !wget -q --show-progress http://www.openslr.org/resources/12/test-clean.tar.gz\n",
        "    !tar -xzf test-clean.tar.gz\n",
        "\n",
        "    # Download noise samples (using ESC-50 as WHAM alternative)\n",
        "    print(\"\\nDownloading noise samples...\")\n",
        "    !wget -q --show-progress https://github.com/karoldvl/ESC-50/archive/master.zip\n",
        "    !unzip -q master.zip\n",
        "\n",
        "    # Collect audio files\n",
        "    speech_files = []\n",
        "    for root, dirs, files in os.walk('LibriSpeech/test-clean'):\n",
        "        for file in files:\n",
        "            if file.endswith('.flac'):\n",
        "                speech_files.append(os.path.join(root, file))\n",
        "\n",
        "    noise_files = []\n",
        "    for root, dirs, files in os.walk('ESC-50-master/audio'):\n",
        "            if file.endswith('.wav'):\n",
        "        for file in files:\n",
        "                noise_files.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"Found {len(speech_files)} speech files and {len(noise_files)} noise files\")\n",
        "\n",
        "    # Create LibriMix-style mixtures\n",
        "    create_mixtures_librimix_style(speech_files, noise_files)\n",
        "\n",
        "    # Clean up\n",
        "    !rm -rf LibriSpeech test-clean.tar.gz ESC-50-master master.zip\n",
        "\n",
        "def create_mixtures_librimix_style(speech_files, noise_files):\n",
        "    \"\"\"\n",
        "    Create mixtures following LibriMix conventions\n",
        "    \"\"\"\n",
        "    import soundfile as sf\n",
        "    import json\n",
        "\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "\n",
        "    # Shuffle files\n",
        "    np.random.shuffle(speech_files)\n",
        "    np.random.shuffle(noise_files)\n",
        "\n",
        "    # Create metadata\n",
        "    metadata = {\n",
        "        'mix_clean': [],\n",
        "        'mix_single': [],\n",
        "        'mix_both': []\n",
        "    }\n",
        "\n",
        "    samples_per_type = 34\n",
        "\n",
        "    # 1. Create mix_clean (2 speakers, no noise)\n",
        "    print(\"\\nCreating mix_clean samples...\")\n",
        "    os.makedirs('sample_audios/mix_clean', exist_ok=True)\n",
        "\n",
        "    for i in tqdm(range(20)):\n",
        "        if i*2+1 < len(speech_files):\n",
        "            # Load two speakers\n",
        "            s1_data, sr1 = librosa.load(speech_files[i*2], sr=16000, mono=True)\n",
        "            s2_data, sr2 = librosa.load(speech_files[i*2+1], sr=16000, mono=True)\n",
        "\n",
        "            # Adjust to 10 seconds\n",
        "            s1_data = adjust_audio_length(s1_data, 10*16000)\n",
        "            s2_data = adjust_audio_length(s2_data, 10*16000)\n",
        "\n",
        "            # Mix with equal energy (LibriMix style)\n",
        "            s1_rms = np.sqrt(np.mean(s1_data**2))\n",
        "            s2_rms = np.sqrt(np.mean(s2_data**2))\n",
        "            s2_data = s2_data * (s1_rms / s2_rms)  # Normalize s2 to s1's level\n",
        "\n",
        "            mixture = s1_data + s2_data\n",
        "\n",
        "            # Prevent clipping\n",
        "            max_val = np.max(np.abs(mixture))\n",
        "            if max_val > 1.0:\n",
        "                mixture = mixture / max_val * 0.95\n",
        "\n",
        "            # Save files\n",
        "            sf.write(f'sample_audios/mix_clean/mixture_{i:03d}.wav', mixture, 16000)\n",
        "            sf.write(f'sample_audios/mix_clean/s1_{i:03d}.wav', s1_data, 16000)\n",
        "            sf.write(f'sample_audios/mix_clean/s2_{i:03d}.wav', s2_data, 16000)\n",
        "\n",
        "            metadata['mix_clean'].append({\n",
        "                'mixture_ID': f'mixture_{i:03d}',\n",
        "                'mixture_path': f'mix_clean/mixture_{i:03d}.wav',\n",
        "                'source_1_path': f'mix_clean/s1_{i:03d}.wav',\n",
        "                'source_2_path': f'mix_clean/s2_{i:03d}.wav',\n",
        "                'noise_path': None,\n",
        "                'SNR': None\n",
        "            })\n",
        "\n",
        "    # 2. Create mix_single (1 speaker + noise)\n",
        "    print(\"\\nCreating mix_single samples...\")\n",
        "    os.makedirs('sample_audios/mix_single', exist_ok=True)\n",
        "\n",
        "    for i in tqdm(range(70)):\n",
        "        if i < len(speech_files) and i < len(noise_files):\n",
        "            # Load speaker and noise\n",
        "            s1_data, sr1 = librosa.load(speech_files[i], sr=16000, mono=True)\n",
        "            noise_data, sr_n = librosa.load(noise_files[i % len(noise_files)], sr=16000, mono=True)\n",
        "\n",
        "            # Adjust to 10 seconds\n",
        "            s1_data = adjust_audio_length(s1_data, 10*16000)\n",
        "            noise_data = adjust_audio_length(noise_data, 10*16000)\n",
        "\n",
        "            # Mix at 10 dB SNR (LibriMix uses various SNRs)\n",
        "            snr_db = 10\n",
        "            s1_rms = np.sqrt(np.mean(s1_data**2))\n",
        "            noise_rms = np.sqrt(np.mean(noise_data**2))\n",
        "            noise_data = noise_data * (s1_rms / noise_rms) * (10**(-snr_db/20))\n",
        "\n",
        "            mixture = s1_data + noise_data\n",
        "\n",
        "            # Prevent clipping\n",
        "            max_val = np.max(np.abs(mixture))\n",
        "            if max_val > 1.0:\n",
        "                mixture = mixture / max_val * 0.95\n",
        "\n",
        "            # Save files\n",
        "            sf.write(f'sample_audios/mix_single/mixture_{i:03d}.wav', mixture, 16000)\n",
        "            sf.write(f'sample_audios/mix_single/s1_{i:03d}.wav', s1_data, 16000)\n",
        "            sf.write(f'sample_audios/mix_single/noise_{i:03d}.wav', noise_data, 16000)\n",
        "\n",
        "            metadata['mix_single'].append({\n",
        "                'mixture_ID': f'mixture_{i:03d}',\n",
        "                'mixture_path': f'mix_single/mixture_{i:03d}.wav',\n",
        "                'source_1_path': f'mix_single/s1_{i:03d}.wav',\n",
        "                'source_2_path': None,\n",
        "                'noise_path': f'mix_single/noise_{i:03d}.wav',\n",
        "                'SNR': snr_db\n",
        "            })\n",
        "\n",
        "    # 3. Create mix_both (2 speakers + noise)\n",
        "    print(\"\\nCreating mix_both samples...\")\n",
        "    os.makedirs('sample_audios/mix_both', exist_ok=True)\n",
        "\n",
        "    for i in tqdm(range(20)):\n",
        "        if i*2+1 < len(speech_files) and i < len(noise_files):\n",
        "            # Load two speakers and noise\n",
        "            s1_data, _ = librosa.load(speech_files[i*2], sr=16000, mono=True)\n",
        "            s2_data, _ = librosa.load(speech_files[i*2+1], sr=16000, mono=True)\n",
        "            noise_data, _ = librosa.load(noise_files[i % len(noise_files)], sr=16000, mono=True)\n",
        "\n",
        "            # Adjust to 10 seconds\n",
        "            s1_data = adjust_audio_length(s1_data, 10*16000)\n",
        "            s2_data = adjust_audio_length(s2_data, 10*16000)\n",
        "            noise_data = adjust_audio_length(noise_data, 10*16000)\n",
        "\n",
        "            # Mix speakers with equal energy\n",
        "            s1_rms = np.sqrt(np.mean(s1_data**2))\n",
        "            s2_rms = np.sqrt(np.mean(s2_data**2))\n",
        "            s2_data = s2_data * (s1_rms / s2_rms)\n",
        "\n",
        "            speech_mix = s1_data + s2_data\n",
        "\n",
        "            # Add noise at 15 dB SNR\n",
        "            snr_db = 15\n",
        "            speech_rms = np.sqrt(np.mean(speech_mix**2))\n",
        "            noise_rms = np.sqrt(np.mean(noise_data**2))\n",
        "            noise_data = noise_data * (speech_rms / noise_rms) * (10**(-snr_db/20))\n",
        "\n",
        "            mixture = speech_mix + noise_data\n",
        "\n",
        "            # Prevent clipping\n",
        "            max_val = np.max(np.abs(mixture))\n",
        "            if max_val > 1.0:\n",
        "                mixture = mixture / max_val * 0.95\n",
        "\n",
        "            # Save files\n",
        "            sf.write(f'sample_audios/mix_both/mixture_{i:03d}.wav', mixture, 16000)\n",
        "            sf.write(f'sample_audios/mix_both/s1_{i:03d}.wav', s1_data, 16000)\n",
        "            sf.write(f'sample_audios/mix_both/s2_{i:03d}.wav', s2_data, 16000)\n",
        "            sf.write(f'sample_audios/mix_both/noise_{i:03d}.wav', noise_data, 16000)\n",
        "\n",
        "            metadata['mix_both'].append({\n",
        "                'mixture_ID': f'mixture_{i:03d}',\n",
        "                'mixture_path': f'mix_both/mixture_{i:03d}.wav',\n",
        "                'source_1_path': f'mix_both/s1_{i:03d}.wav',\n",
        "                'source_2_path': f'mix_both/s2_{i:03d}.wav',\n",
        "                'noise_path': f'mix_both/noise_{i:03d}.wav',\n",
        "                'SNR': snr_db\n",
        "            })\n",
        "\n",
        "    # Save metadata\n",
        "    with open('sample_audios/librimix_metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"\\n✓ Created LibriMix-style dataset:\")\n",
        "    print(f\"  - {len(metadata['mix_clean'])} mix_clean samples (2 speakers, no noise)\")\n",
        "    print(f\"  - {len(metadata['mix_single'])} mix_single samples (1 speaker + noise)\")\n",
        "    print(f\"  - {len(metadata['mix_both'])} mix_both samples (2 speakers + noise)\")\n",
        "    print(f\"\\nTotal: ~100 samples following LibriMix conventions\")\n",
        "\n",
        "def adjust_audio_length(audio, target_length):\n",
        "    \"\"\"Adjust audio to target length by truncating or padding\"\"\"\n",
        "    if len(audio) > target_length:\n",
        "        return audio[:target_length]\n",
        "    else:\n",
        "        return np.pad(audio, (0, target_length - len(audio)), mode='constant')\n",
        "\n",
        "# Run the simplified LibriMix creation\n",
        "create_simple_librimix_alternative()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haSTkCIh7Lw3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Initialize the model (you can choose different model sizes)\n",
        "# Available models: tiny, base, small, medium, large, large-v2, large-v3\n",
        "model_size = \"base\"  # Change this to your preferred model\n",
        "\n",
        "# Initialize with GPU support if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "\n",
        "print(f\"Loading Whisper model: {model_size}\")\n",
        "print(f\"Device: {device}, Compute type: {compute_type}\")\n",
        "\n",
        "model = WhisperModel(model_size, device=device, compute_type=compute_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dWcaBOvHDUj"
      },
      "outputs": [],
      "source": [
        "!pip install -q jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEVx1kzHtG7O"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import json\n",
        "from jiwer import wer, cer\n",
        "def test_single_mixture_type(mix_type, model_size=\"base\", model=None):\n",
        "    \"\"\"\n",
        "    Test a single mixture type to manage memory better\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Testing {mix_type} samples...\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Initialize model if not provided\n",
        "    if model is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "        print(f\"Loading model: {model_size} on {device}\")\n",
        "        model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
        "\n",
        "    # Load metadata\n",
        "    with open('sample_audios/librimix_metadata.json', 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    samples = metadata[mix_type]\n",
        "    results = []\n",
        "\n",
        "    for sample in tqdm(samples, desc=f\"Processing {mix_type}\"):\n",
        "        # Paths\n",
        "        mixture_path = f\"sample_audios/{sample['mixture_path']}\"\n",
        "        s1_path = f\"sample_audios/{sample['source_1_path']}\"\n",
        "        s2_path = f\"sample_audios/{sample['source_2_path']}\" if sample['source_2_path'] else None\n",
        "\n",
        "        # Get ground truth by transcribing clean sources\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Transcribe source 1\n",
        "        segments_s1, _ = model.transcribe(s1_path, beam_size=5)\n",
        "        ground_truth_s1 = \" \".join([segment.text.strip() for segment in segments_s1])\n",
        "\n",
        "        # Transcribe source 2 if exists\n",
        "        if s2_path:\n",
        "            segments_s2, _ = model.transcribe(s2_path, beam_size=5)\n",
        "            ground_truth_s2 = \" \".join([segment.text.strip() for segment in segments_s2])\n",
        "            ground_truth = f\"{ground_truth_s1} {ground_truth_s2}\"\n",
        "        else:\n",
        "            ground_truth = ground_truth_s1\n",
        "\n",
        "        # Transcribe mixture\n",
        "        segments_mix, _ = model.transcribe(mixture_path, beam_size=5)\n",
        "        transcription = \" \".join([segment.text.strip() for segment in segments_mix])\n",
        "\n",
        "        inference_time = time.time() - start_time\n",
        "\n",
        "        # Calculate metrics\n",
        "        if ground_truth.strip() and transcription.strip():\n",
        "            word_error_rate = wer(ground_truth, transcription)\n",
        "            char_error_rate = cer(ground_truth, transcription)\n",
        "        else:\n",
        "            word_error_rate = 1.0\n",
        "            char_error_rate = 1.0\n",
        "\n",
        "        # Store results\n",
        "        result = {\n",
        "            'mixture_type': mix_type,\n",
        "            'sample_id': sample['mixture_ID'],\n",
        "            'ground_truth': ground_truth,\n",
        "            'transcription': transcription,\n",
        "            'wer': word_error_rate,\n",
        "            'cer': char_error_rate,\n",
        "            'inference_time': inference_time,\n",
        "            'snr': sample.get('SNR', 'N/A')\n",
        "        }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "        # Clear memory periodically\n",
        "        if len(results) % 10 == 0:\n",
        "            gc.collect()\n",
        "\n",
        "    # Convert to DataFrame and save chunk\n",
        "    df_chunk = pd.DataFrame(results)\n",
        "    chunk_file = f'whisper_baseline_{model_size}_{mix_type}_results.csv'\n",
        "    df_chunk.to_csv(chunk_file, index=False)\n",
        "\n",
        "    # Print statistics for this chunk\n",
        "    print(f\"\\n{mix_type} Results:\")\n",
        "    print(f\"  Samples: {len(df_chunk)}\")\n",
        "    print(f\"  Average WER: {df_chunk['wer'].mean():.3f} (±{df_chunk['wer'].std():.3f})\")\n",
        "    print(f\"  Average CER: {df_chunk['cer'].mean():.3f} (±{df_chunk['cer'].std():.3f})\")\n",
        "    print(f\"  Average time: {df_chunk['inference_time'].mean():.2f}s\")\n",
        "\n",
        "    # Show sample\n",
        "    if len(df_chunk) > 0:\n",
        "        sample = df_chunk.iloc[0]\n",
        "        print(f\"\\n  Sample transcription:\")\n",
        "        print(f\"  Ground truth: {sample['ground_truth'][:80]}...\")\n",
        "        print(f\"  Transcription: {sample['transcription'][:80]}...\")\n",
        "\n",
        "    print(f\"\\n✓ Saved {mix_type} results to '{chunk_file}'\")\n",
        "\n",
        "    return df_chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNc6PBD7tIoV"
      },
      "outputs": [],
      "source": [
        "df_clean = test_single_mixture_type('mix_clean', model_size='base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1pxK1Tiw0GH"
      },
      "outputs": [],
      "source": [
        "df_single = test_single_mixture_type('mix_single', model_size='base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSsR0h_Nz8CL"
      },
      "outputs": [],
      "source": [
        "df_both = test_single_mixture_type('mix_both', model_size='base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA1FVfvJZTyk"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/karoldvl/ESC-50/archive/master.zip -O esc50.zip\n",
        "!unzip -q esc50.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wUjgsa11goV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def create_noise_only_samples(num_samples=10, duration_seconds=10):\n",
        "    \"\"\"\n",
        "    Create noise-only audio samples from ESC-50 and synthetic white noise.\n",
        "\n",
        "    Args:\n",
        "        num_samples: Total number of samples to generate\n",
        "        duration_seconds: Duration of each sample in seconds\n",
        "\n",
        "    Returns:\n",
        "        List of file paths created\n",
        "    \"\"\"\n",
        "    print(\"Creating noise-only samples...\")\n",
        "    os.makedirs('sample_audios/noise_only', exist_ok=True)\n",
        "\n",
        "    # Load metadata\n",
        "    meta_path = 'ESC-50-master/meta/esc50.csv'\n",
        "    if not os.path.exists(meta_path):\n",
        "        raise FileNotFoundError(\"esc50.csv not found. Download and place it in ESC-50-master/meta/\")\n",
        "\n",
        "    df_meta = pd.read_csv(meta_path)\n",
        "\n",
        "    # Define broader noise groups and match their ESC-50 categories\n",
        "    noise_category_groups = {\n",
        "        'environmental': ['rain', 'wind', 'thunderstorm', 'water_drops'],\n",
        "        'urban': ['car_horn', 'siren', 'engine', 'train'],\n",
        "        'indoor': ['clock_tick', 'keyboard_typing', 'vacuum_cleaner'],\n",
        "        'nature': ['insects', 'crow', 'dog', 'rooster', 'chirping_birds'],\n",
        "        'human': ['breathing', 'coughing', 'snoring', 'laughing']\n",
        "    }\n",
        "\n",
        "    # Flatten desired ESC-50 categories\n",
        "    desired_categories = [cat for group in noise_category_groups.values() for cat in group]\n",
        "\n",
        "    # Filter ESC-50 metadata\n",
        "    df_filtered = df_meta[df_meta['category'].isin(desired_categories)]\n",
        "\n",
        "    # Map category to broader group\n",
        "    def get_group(category):\n",
        "        for group, members in noise_category_groups.items():\n",
        "            if category in members:\n",
        "                return group\n",
        "        return \"unknown\"\n",
        "\n",
        "    # Shuffle and prepare paths\n",
        "    entries = df_filtered.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    created_files = []\n",
        "    metadata = []\n",
        "    sample_count = 0\n",
        "    sample_rate = 16000\n",
        "    target_duration_ms = duration_seconds * 1000\n",
        "\n",
        "    # Reserve a few samples for white noise\n",
        "    real_needed = num_samples - 3\n",
        "\n",
        "    for _, row in entries.iterrows():\n",
        "        if sample_count >= real_needed:\n",
        "            break\n",
        "\n",
        "        wav_path = os.path.join(\"ESC-50-master/audio\", row[\"filename\"])\n",
        "        if not os.path.exists(wav_path):\n",
        "            continue\n",
        "\n",
        "        category = row[\"category\"]\n",
        "        group = get_group(category)\n",
        "\n",
        "        noise_audio = AudioSegment.from_wav(wav_path).set_frame_rate(sample_rate).set_channels(1)\n",
        "\n",
        "        if len(noise_audio) < target_duration_ms:\n",
        "            loops = (target_duration_ms // len(noise_audio)) + 1\n",
        "            noise_audio = noise_audio * loops\n",
        "\n",
        "        noise_audio = noise_audio[:target_duration_ms]\n",
        "        volume_adj = random.choice([-10, -5, 0, 5])\n",
        "        noise_audio += volume_adj\n",
        "\n",
        "        out_path = f'sample_audios/noise_only/{group}_{sample_count:03d}.wav'\n",
        "        noise_audio.export(out_path, format='wav')\n",
        "        created_files.append(out_path)\n",
        "\n",
        "        metadata.append({\n",
        "            'filename': os.path.basename(out_path),\n",
        "            'group': group,\n",
        "            'esc_category': category,\n",
        "            'source_file': row[\"filename\"],\n",
        "            'volume_adjustment_db': volume_adj,\n",
        "            'duration_seconds': duration_seconds\n",
        "        })\n",
        "\n",
        "        sample_count += 1\n",
        "\n",
        "    # Add synthetic white noise\n",
        "    for i in range(3):\n",
        "        if sample_count >= num_samples:\n",
        "            break\n",
        "\n",
        "        samples = int(duration_seconds * sample_rate)\n",
        "        noise_level = random.choice([0.01, 0.05, 0.1])\n",
        "        white_noise = np.random.normal(0, noise_level, samples).astype(np.float32)\n",
        "        white_noise = np.clip(white_noise, -1, 1)\n",
        "\n",
        "        out_path = f'sample_audios/noise_only/white_noise_{sample_count:03d}.wav'\n",
        "        sf.write(out_path, white_noise, sample_rate)\n",
        "        created_files.append(out_path)\n",
        "\n",
        "        metadata.append({\n",
        "            'filename': os.path.basename(out_path),\n",
        "            'group': 'synthetic',\n",
        "            'esc_category': 'generated_white_noise',\n",
        "            'source_file': 'none',\n",
        "            'volume_adjustment_db': 0,\n",
        "            'noise_level': noise_level,\n",
        "            'duration_seconds': duration_seconds\n",
        "        })\n",
        "\n",
        "        sample_count += 1\n",
        "\n",
        "    # Save metadata\n",
        "    with open('sample_audios/noise_only/metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"\\n✓ Created {len(created_files)} noise-only samples\")\n",
        "    df_summary = pd.DataFrame(metadata)\n",
        "    print(df_summary['group'].value_counts())\n",
        "\n",
        "    return created_files\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6j-4kzeO3q2"
      },
      "outputs": [],
      "source": [
        "def test_noise_only_baseline(model_size=\"base\"):\n",
        "    \"\"\"\n",
        "    Test Whisper on noise-only samples to see what it hallucinates (baseline only)\n",
        "\n",
        "    Args:\n",
        "        model_size: Whisper model size\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with results\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Testing Whisper on Noise-Only Samples (Baseline)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Initialize model\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "    print(f\"Loading model: {model_size} on {device}\")\n",
        "    model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
        "\n",
        "    # Load metadata\n",
        "    with open('sample_audios/noise_only/metadata.json', 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for sample in tqdm(metadata, desc=\"Processing noise-only samples\"):\n",
        "        audio_path = f\"sample_audios/noise_only/{sample['filename']}\"\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Direct transcription\n",
        "        segments, _ = model.transcribe(audio_path, beam_size=5)\n",
        "        transcription = \" \".join([segment.text.strip() for segment in segments])\n",
        "\n",
        "        inference_time = time.time() - start_time\n",
        "\n",
        "        # For noise-only, any transcription is a hallucination\n",
        "        is_hallucination = len(transcription.strip()) > 0\n",
        "        word_count = len(transcription.split()) if is_hallucination else 0\n",
        "        char_count = len(transcription.strip()) if is_hallucination else 0\n",
        "\n",
        "        result = {\n",
        "            'filename': sample['filename'],\n",
        "            'category': sample.get('group', sample.get('category', 'unknown')),\n",
        "            'transcription': transcription,\n",
        "            'is_hallucination': is_hallucination,\n",
        "            'word_count': word_count,\n",
        "            'char_count': char_count,\n",
        "            'inference_time': inference_time,\n",
        "            'volume_adjustment_db': sample.get('volume_adjustment_db', 0)\n",
        "        }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df_results = pd.DataFrame(results)\n",
        "\n",
        "    # Analysis\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"NOISE-ONLY BASELINE RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Overall hallucination rate\n",
        "    hallucination_rate = df_results['is_hallucination'].mean() * 100\n",
        "    avg_words = df_results[df_results['is_hallucination']]['word_count'].mean() if any(df_results['is_hallucination']) else 0\n",
        "\n",
        "    print(f\"\\nOverall Results:\")\n",
        "    print(f\"  Hallucination rate: {hallucination_rate:.1f}%\")\n",
        "    print(f\"  Average words when hallucinating: {avg_words:.1f}\")\n",
        "    print(f\"  Total samples tested: {len(df_results)}\")\n",
        "\n",
        "    # Hallucination by noise category\n",
        "    print(\"\\nHallucination Rate by Noise Category:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for category in df_results['category'].unique():\n",
        "        cat_data = df_results[df_results['category'] == category]\n",
        "        cat_hall_rate = cat_data['is_hallucination'].mean() * 100\n",
        "        cat_samples = len(cat_data)\n",
        "        print(f\"{category}: {cat_hall_rate:.1f}% ({cat_samples} samples)\")\n",
        "\n",
        "    # Sample hallucinations\n",
        "    print(\"\\nSample Hallucinations:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    hallucinations = df_results[df_results['is_hallucination']]\n",
        "    if len(hallucinations) > 0:\n",
        "        for idx, row in hallucinations.head(10).iterrows():\n",
        "            print(f\"\\n{row['category']} noise → \\\"{row['transcription']}\\\"\")\n",
        "    else:\n",
        "        print(\"No hallucinations detected!\")\n",
        "\n",
        "    # Save results\n",
        "    output_file = f'whisper_{model_size}_noise_only_baseline.csv'\n",
        "    df_results.to_csv(output_file, index=False)\n",
        "    print(f\"\\n✓ Results saved to '{output_file}'\")\n",
        "\n",
        "    return df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7RK-FGrPELY"
      },
      "outputs": [],
      "source": [
        "def test_noise_only_complete(model_size=\"base\"):\n",
        "    \"\"\"\n",
        "    Complete noise-only test pipeline (baseline only)\n",
        "    \"\"\"\n",
        "    # Step 1: Create noise-only samples if they don't exist\n",
        "    #if not os.path.exists('sample_audios/noise_only/metadata.json'):\n",
        "    create_noise_only_samples(num_samples=20, duration_seconds=10)\n",
        "   #\n",
        "        #print(\"Noise-only samples already exist\")\n",
        "\n",
        "    # Step 2: Test with Whisper (baseline only)\n",
        "    df_results = test_noise_only_baseline(model_size=model_size)\n",
        "\n",
        "    # Step 3: Visualize results\n",
        "    #visualize_noise_baseline_results(df_results, model_size=model_size)\n",
        "\n",
        "    # Step 4: Key findings\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"KEY FINDINGS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Category with highest hallucination rate\n",
        "    hall_by_cat = df_results.groupby('category')['is_hallucination'].mean()\n",
        "    worst_cat = hall_by_cat.idxmax()\n",
        "    worst_rate = hall_by_cat.max() * 100\n",
        "\n",
        "    best_cat = hall_by_cat.idxmin()\n",
        "    best_rate = hall_by_cat.min() * 100\n",
        "\n",
        "    print(f\"\\nMost problematic noise type: {worst_cat} ({worst_rate:.1f}% hallucination)\")\n",
        "    print(f\"Least problematic noise type: {best_cat} ({best_rate:.1f}% hallucination)\")\n",
        "\n",
        "    # Volume effect\n",
        "    if 'volume_adjustment_db' in df_results.columns:\n",
        "        vol_corr = df_results[['volume_adjustment_db', 'is_hallucination']].corr().iloc[0, 1]\n",
        "        print(f\"\\nVolume correlation with hallucination: {vol_corr:.3f}\")\n",
        "\n",
        "    return df_results\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Run complete noise-only test\n",
        "    results = test_noise_only_complete(model_size=\"base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IXA18r2jKCp"
      },
      "outputs": [],
      "source": [
        "# Test Noise-Only Audio with Filters to Reduce Hallucinations\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from faster_whisper import WhisperModel\n",
        "import noisereduce as nr\n",
        "from scipy.signal import butter, filtfilt, wiener\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import torch\n",
        "import tempfile\n",
        "\n",
        "# Install required packages\n",
        "# !pip install -q noisereduce\n",
        "\n",
        "class SimpleNoiseFilter:\n",
        "    \"\"\"\n",
        "    Simple noise filtering methods for reducing hallucinations\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sample_rate=16000):\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def spectral_gating(self, audio_data, sr):\n",
        "        \"\"\"Apply spectral gating noise reduction\"\"\"\n",
        "        return nr.reduce_noise(y=audio_data, sr=sr, prop_decrease=1.0)\n",
        "\n",
        "    def wiener_filter(self, audio_data, noise_power=0.1):\n",
        "        \"\"\"Apply Wiener filter\"\"\"\n",
        "        return wiener(audio_data, noise=noise_power)\n",
        "\n",
        "    def spectral_subtraction(self, audio_data, sr, noise_factor=0.1):\n",
        "        \"\"\"Apply spectral subtraction\"\"\"\n",
        "        # STFT\n",
        "        stft = librosa.stft(audio_data, n_fft=2048, hop_length=512)\n",
        "        magnitude = np.abs(stft)\n",
        "        phase = np.angle(stft)\n",
        "\n",
        "        # Estimate noise from first 0.5 seconds\n",
        "        noise_frames = int(0.5 * sr / 512)\n",
        "        noise_magnitude = np.mean(magnitude[:, :noise_frames], axis=1, keepdims=True)\n",
        "\n",
        "        # Subtract noise\n",
        "        clean_magnitude = magnitude - noise_factor * noise_magnitude\n",
        "        clean_magnitude = np.maximum(clean_magnitude, 0.002 * magnitude)\n",
        "\n",
        "        # Reconstruct\n",
        "        clean_stft = clean_magnitude * np.exp(1j * phase)\n",
        "        clean_audio = librosa.istft(clean_stft, hop_length=512)\n",
        "\n",
        "        return clean_audio\n",
        "\n",
        "    def aggressive_filter(self, audio_data, sr):\n",
        "        \"\"\"Apply aggressive filtering to minimize hallucinations\"\"\"\n",
        "        # Step 1: Heavy spectral gating\n",
        "        filtered = nr.reduce_noise(y=audio_data, sr=sr, prop_decrease=1.5, stationary=True)\n",
        "\n",
        "        # Step 2: Aggressive high-pass filter (remove low frequencies)\n",
        "        nyquist = sr / 2\n",
        "        cutoff = 300 / nyquist  # 300 Hz cutoff\n",
        "        b, a = butter(5, cutoff, btype='high')\n",
        "        filtered = filtfilt(b, a, filtered)\n",
        "\n",
        "        # Step 3: Amplitude gating (silence very quiet parts)\n",
        "        threshold = np.max(np.abs(filtered)) * 0.1\n",
        "        mask = np.abs(filtered) > threshold\n",
        "        filtered = filtered * mask\n",
        "\n",
        "        return filtered\n",
        "\n",
        "def test_noise_with_filters(model_size=\"base\", filter_methods=['none', 'spectral_gating', 'aggressive']):\n",
        "    \"\"\"\n",
        "    Test noise-only samples with different filters to measure hallucination reduction\n",
        "\n",
        "    Args:\n",
        "        model_size: Whisper model size\n",
        "        filter_methods: List of filter methods to test\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with results\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Testing Noise-Only Samples with Filters\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Initialize model\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
        "    print(f\"Loading model: {model_size} on {device}\")\n",
        "    model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
        "\n",
        "    # Initialize filter\n",
        "    noise_filter = SimpleNoiseFilter()\n",
        "\n",
        "    # Load metadata\n",
        "    with open('sample_audios/noise_only/metadata.json', 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Test each sample with each filter\n",
        "    for sample in tqdm(metadata, desc=\"Processing samples\"):\n",
        "        audio_path = f\"sample_audios/noise_only/{sample['filename']}\"\n",
        "\n",
        "        # Load audio once\n",
        "        audio_data, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "        for filter_method in filter_methods:\n",
        "            start_time = time.time()\n",
        "\n",
        "            if filter_method == 'none':\n",
        "                # Direct transcription\n",
        "                segments, _ = model.transcribe(audio_path, beam_size=5)\n",
        "            else:\n",
        "                # Apply filter\n",
        "                if filter_method == 'spectral_gating':\n",
        "                    filtered_audio = noise_filter.spectral_gating(audio_data, sr)\n",
        "                elif filter_method == 'wiener':\n",
        "                    filtered_audio = noise_filter.wiener_filter(audio_data)\n",
        "                elif filter_method == 'spectral_subtraction':\n",
        "                    filtered_audio = noise_filter.spectral_subtraction(audio_data, sr)\n",
        "                elif filter_method == 'aggressive':\n",
        "                    filtered_audio = noise_filter.aggressive_filter(audio_data, sr)\n",
        "                else:\n",
        "                    filtered_audio = audio_data\n",
        "\n",
        "                # Normalize\n",
        "                max_val = np.max(np.abs(filtered_audio))\n",
        "                if max_val > 0:\n",
        "                    filtered_audio = filtered_audio / max_val * 0.95\n",
        "\n",
        "                # Save to temp file and transcribe\n",
        "                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
        "                    sf.write(tmp.name, filtered_audio, sr)\n",
        "                    segments, _ = model.transcribe(tmp.name, beam_size=5)\n",
        "                    os.unlink(tmp.name)\n",
        "\n",
        "            transcription = \" \".join([segment.text.strip() for segment in segments])\n",
        "            inference_time = time.time() - start_time\n",
        "\n",
        "            # Analyze results\n",
        "            is_hallucination = len(transcription.strip()) > 0\n",
        "            word_count = len(transcription.split()) if is_hallucination else 0\n",
        "            char_count = len(transcription.strip()) if is_hallucination else 0\n",
        "\n",
        "            result = {\n",
        "                'filename': sample['filename'],\n",
        "                'category': sample.get('group', sample.get('category', 'unknown')),\n",
        "                'filter_method': filter_method,\n",
        "                'transcription': transcription,\n",
        "                'is_hallucination': is_hallucination,\n",
        "                'word_count': word_count,\n",
        "                'char_count': char_count,\n",
        "                'inference_time': inference_time,\n",
        "                'volume_adjustment_db': sample.get('volume_adjustment_db', 0)\n",
        "            }\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def analyze_filter_effectiveness(df_results):\n",
        "    \"\"\"\n",
        "    Analyze how effective filters are at reducing hallucinations\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FILTER EFFECTIVENESS ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Overall hallucination rates by filter\n",
        "    print(\"\\nHallucination Rates by Filter Method:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    filter_stats = []\n",
        "\n",
        "    for filter_method in df_results['filter_method'].unique():\n",
        "        filter_data = df_results[df_results['filter_method'] == filter_method]\n",
        "        hall_rate = filter_data['is_hallucination'].mean() * 100\n",
        "        avg_words = filter_data[filter_data['is_hallucination']]['word_count'].mean() if any(filter_data['is_hallucination']) else 0\n",
        "\n",
        "        filter_stats.append({\n",
        "            'Filter': filter_method,\n",
        "            'Hallucination Rate (%)': f\"{hall_rate:.1f}\",\n",
        "            'Avg Words': f\"{avg_words:.1f}\",\n",
        "            'Samples': len(filter_data)\n",
        "        })\n",
        "\n",
        "        print(f\"{filter_method}:\")\n",
        "        print(f\"  Hallucination rate: {hall_rate:.1f}%\")\n",
        "        print(f\"  Average words when hallucinating: {avg_words:.1f}\")\n",
        "\n",
        "    # Calculate improvement\n",
        "    baseline_rate = df_results[df_results['filter_method'] == 'none']['is_hallucination'].mean() * 100\n",
        "\n",
        "    print(\"\\nImprovement over Baseline:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    improvements = []\n",
        "\n",
        "    for filter_method in df_results['filter_method'].unique():\n",
        "        if filter_method != 'none':\n",
        "            filter_rate = df_results[df_results['filter_method'] == filter_method]['is_hallucination'].mean() * 100\n",
        "            improvement = baseline_rate - filter_rate\n",
        "            relative_improvement = (improvement / baseline_rate * 100) if baseline_rate > 0 else 0\n",
        "\n",
        "            improvements.append({\n",
        "                'Filter': filter_method,\n",
        "                'Reduction': f\"{improvement:.1f}%\",\n",
        "                'Relative Improvement': f\"{relative_improvement:.1f}%\"\n",
        "            })\n",
        "\n",
        "            print(f\"{filter_method}: {improvement:.1f}% absolute reduction ({relative_improvement:.1f}% relative)\")\n",
        "\n",
        "    # Best filter by category\n",
        "    print(\"\\nBest Filter by Noise Category:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for category in df_results['category'].unique():\n",
        "        cat_data = df_results[df_results['category'] == category]\n",
        "\n",
        "        # Find filter with lowest hallucination rate\n",
        "        hall_by_filter = cat_data.groupby('filter_method')['is_hallucination'].mean()\n",
        "        best_filter = hall_by_filter.idxmin()\n",
        "        best_rate = hall_by_filter.min() * 100\n",
        "        baseline = hall_by_filter.get('none', 1) * 100\n",
        "\n",
        "        print(f\"{category}: {best_filter} ({best_rate:.1f}% vs {baseline:.1f}% baseline)\")\n",
        "\n",
        "    return pd.DataFrame(filter_stats), pd.DataFrame(improvements)\n",
        "\n",
        "def test_noise_complete_with_filters(model_size=\"base\"):\n",
        "    \"\"\"\n",
        "    Complete test pipeline with filters\n",
        "    \"\"\"\n",
        "    # Define filters to test\n",
        "    filter_methods = ['none', 'spectral_gating', 'wiener', 'spectral_subtraction', 'aggressive']\n",
        "\n",
        "    # Test with all filters\n",
        "    print(\"Testing multiple noise reduction filters...\")\n",
        "    df_results = test_noise_with_filters(model_size, filter_methods)\n",
        "\n",
        "    # Analyze results\n",
        "    filter_stats, improvements = analyze_filter_effectiveness(df_results)\n",
        "\n",
        "    # Save detailed results\n",
        "    output_file = f'whisper_{model_size}_noise_filtered_comparison.csv'\n",
        "    df_results.to_csv(output_file, index=False)\n",
        "    print(f\"\\n✓ Detailed results saved to '{output_file}'\")\n",
        "\n",
        "    # Save summary\n",
        "    summary_file = f'whisper_{model_size}_filter_summary.csv'\n",
        "    filter_stats.to_csv(summary_file, index=False)\n",
        "    print(f\"✓ Summary saved to '{summary_file}'\")\n",
        "\n",
        "    # Key findings\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"KEY FINDINGS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Best overall filter\n",
        "    hall_by_filter = df_results.groupby('filter_method')['is_hallucination'].mean()\n",
        "    best_filter = hall_by_filter.idxmin()\n",
        "    best_rate = hall_by_filter.min() * 100\n",
        "\n",
        "    print(f\"\\nBest overall filter: {best_filter}\")\n",
        "    print(f\"Hallucination rate: {best_rate:.1f}%\")\n",
        "\n",
        "    # Sample hallucinations that were fixed\n",
        "    baseline_hall = df_results[(df_results['filter_method'] == 'none') & (df_results['is_hallucination'])]\n",
        "\n",
        "    if len(baseline_hall) > 0:\n",
        "        print(\"\\nExamples of hallucinations prevented by filtering:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        for idx, baseline_row in baseline_hall.head(3).iterrows():\n",
        "            # Find same file with best filter\n",
        "            filtered_row = df_results[\n",
        "                (df_results['filename'] == baseline_row['filename']) &\n",
        "                (df_results['filter_method'] == best_filter)\n",
        "            ].iloc[0]\n",
        "\n",
        "            if not filtered_row['is_hallucination']:\n",
        "                print(f\"\\n{baseline_row['category']} noise:\")\n",
        "                print(f\"  Baseline: \\\"{baseline_row['transcription']}\\\"\")\n",
        "                print(f\"  {best_filter}: (no hallucination)\")\n",
        "\n",
        "    return df_results\n",
        "\n",
        "# Quick comparison function\n",
        "def quick_filter_comparison(audio_file, model_size=\"base\"):\n",
        "    \"\"\"\n",
        "    Quick test of all filters on a single audio file\n",
        "    \"\"\"\n",
        "    print(f\"\\nTesting filters on: {audio_file}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    model = WhisperModel(model_size)\n",
        "    noise_filter = SimpleNoiseFilter()\n",
        "\n",
        "    # Load audio\n",
        "    audio_data, sr = librosa.load(audio_file, sr=16000)\n",
        "\n",
        "    # Test each filter\n",
        "    filters = {\n",
        "        'none': audio_data,\n",
        "        'spectral_gating': noise_filter.spectral_gating(audio_data, sr),\n",
        "        'wiener': noise_filter.wiener_filter(audio_data),\n",
        "        'aggressive': noise_filter.aggressive_filter(audio_data, sr)\n",
        "    }\n",
        "\n",
        "    for name, filtered_audio in filters.items():\n",
        "        if name == 'none':\n",
        "            segments, _ = model.transcribe(audio_file)\n",
        "        else:\n",
        "            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
        "                sf.write(tmp.name, filtered_audio, sr)\n",
        "                segments, _ = model.transcribe(tmp.name)\n",
        "                os.unlink(tmp.name)\n",
        "\n",
        "        transcription = \" \".join([s.text.strip() for s in segments])\n",
        "        print(f\"\\n{name}: \\\"{transcription}\\\"\" if transcription else f\"\\n{name}: (no hallucination)\")\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Run complete test with filters\n",
        "    results = test_noise_complete_with_filters(model_size=\"base\")\n",
        "\n",
        "    # Optional: Quick test on single file\n",
        "    # quick_filter_comparison(\"sample_audios/noise_only/human_001.wav\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tv2e6Y3JxNG"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqQj1Bv2JzV8"
      },
      "outputs": [],
      "source": [
        "! add-apt-repository -y ppa:savoury1/ffmpeg4\n",
        "! apt-get -qq install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_dJyTUBJ00H"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPuXem4TJ2PI"
      },
      "outputs": [],
      "source": [
        "!rm -rf WhisperHallu\n",
        "!git clone https://github.com/EtienneAb3d/WhisperHallu.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHDjIZowJ4DB"
      },
      "outputs": [],
      "source": [
        "!pip install -U demucs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_IlQBi3J5j3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"WhisperHallu\")\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGjuDQZtJ7Cw"
      },
      "outputs": [],
      "source": [
        "!pip install \"faster-whisper @ git+https://github.com/guillaumekln/faster-whisper@master#faster-whisper[conversion]\"\n",
        "\n",
        "!ct2-transformers-converter --model openai/whisper-medium --output_dir whisper-medium-ct2 --quantization float16\n",
        "!ct2-transformers-converter --model openai/whisper-large --output_dir whisper-large-ct2 --quantization float16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OqShRQuJ8vI"
      },
      "outputs": [],
      "source": [
        "!pip3 install torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrywTVsVJ-cz"
      },
      "outputs": [],
      "source": [
        "!ls -lahtr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8ad449BRsSg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "folder = '../sample_audios/noise_only'\n",
        "meta = []\n",
        "\n",
        "for fname in os.listdir(folder):\n",
        "    if fname.endswith('.wav'):\n",
        "        category = fname.split('_')[0]\n",
        "        meta.append({\"filename\": fname, \"category\": category})\n",
        "\n",
        "# Overwrite metadata.json with only existing files\n",
        "with open(os.path.join(folder, 'metadata.json'), 'w') as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(f\"✅ metadata.json created with {len(meta)} valid entries!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8ySWEm6LvGY"
      },
      "outputs": [],
      "source": [
        "# Test WhisperHallu on Noise-Only Audio\n",
        "\n",
        "# First install WhisperHallu\n",
        "# !pip install -q git+https://github.com/Mageswaran1989/WhisperHallu\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# Import WhisperHallu\n",
        "try:\n",
        "    from transcribeHallu import loadModel, transcribePrompt\n",
        "except ImportError:\n",
        "    print(\"Please install WhisperHallu first:\")\n",
        "    print(\"!pip install git+https://github.com/Mageswaran1989/WhisperHallu\")\n",
        "    raise\n",
        "\n",
        "def test_whisperhallu_on_noise(model_size=\"base\", max_samples=10):\n",
        "    \"\"\"\n",
        "    Test WhisperHallu on noise-only samples to see if it reduces hallucinations\n",
        "\n",
        "    Args:\n",
        "        model_size: Model size (base, small, medium, large)\n",
        "        max_samples: Number of samples to test\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with results\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Testing WhisperHallu on Noise-Only Audio\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Load WhisperHallu model\n",
        "    device = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Loading WhisperHallu model: {model_size} on device: {device}\")\n",
        "    loadModel(device, modelSize=model_size)\n",
        "\n",
        "    # Define anti-hallucination prompt for English\n",
        "    # This prompt is designed to prevent common hallucinations\n",
        "    anti_hallucination_prompt = (\n",
        "        \"Whisper, Ok. \"\n",
        "        \"This is just noise, no speech. \"\n",
        "        \"Ok, Whisper. Whisper, Ok. \"\n",
        "        \"Ok, Whisper. Whisper, Ok. \"\n",
        "        \"Please do not transcribe anything from noise. \"\n",
        "        \"This is to avoid hallucinations. \"\n",
        "        \"Ok, Whisper. \"\n",
        "    )\n",
        "\n",
        "    # Alternative prompt that explicitly mentions silence\n",
        "    silence_prompt = (\n",
        "        \"Whisper, Ok. \"\n",
        "        \"Silence. No words. Just background noise. \"\n",
        "        \"Ok, Whisper. Whisper, Ok. \"\n",
        "        \"Empty audio. No speech detected. \"\n",
        "        \"Ok, Whisper. Whisper, Ok. \"\n",
        "        \"Please transcribe only actual speech. \"\n",
        "        \"Ok, Whisper. \"\n",
        "    )\n",
        "\n",
        "    # Load metadata\n",
        "    with open('sample_audios/noise_only/metadata.json', 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    # Limit samples\n",
        "    metadata = metadata[:max_samples]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Test each noise sample with different prompts\n",
        "    prompts = {\n",
        "        'no_prompt': \"\",  # Baseline - empty prompt\n",
        "        'anti_hallucination': anti_hallucination_prompt,\n",
        "        'silence_prompt': silence_prompt\n",
        "    }\n",
        "\n",
        "    for sample in tqdm(metadata, desc=\"Processing noise samples\"):\n",
        "        audio_path = os.path.join(\"sample_audios\", \"noise_only\", sample[\"filename\"])\n",
        "\n",
        "        if not os.path.exists(audio_path):\n",
        "            print(f\"File not found: {audio_path}\")\n",
        "            continue\n",
        "\n",
        "        for prompt_name, prompt_text in prompts.items():\n",
        "            start_time = time.time()\n",
        "\n",
        "            try:\n",
        "                # Transcribe with WhisperHallu\n",
        "                result = transcribePrompt(\n",
        "                    path=audio_path,\n",
        "                    lng=\"en\",\n",
        "                    prompt=prompt_text\n",
        "                )\n",
        "\n",
        "                # Extract transcription\n",
        "                # WhisperHallu returns a dict with 'text' key\n",
        "                if isinstance(result, dict) and 'text' in result:\n",
        "                    transcription = result['text'].strip()\n",
        "                else:\n",
        "                    transcription = str(result).strip()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {audio_path} using {prompt_name}: {e}\")\n",
        "                transcription = \"\"\n",
        "\n",
        "            inference_time = time.time() - start_time\n",
        "\n",
        "            # Check for hallucination\n",
        "            is_hallucination = len(transcription) > 0\n",
        "            word_count = len(transcription.split()) if is_hallucination else 0\n",
        "\n",
        "            # Store result\n",
        "            results.append({\n",
        "                'filename': sample['filename'],\n",
        "                'category': sample.get('group', sample.get('category', 'unknown')),\n",
        "                'prompt_type': prompt_name,\n",
        "                'transcription': transcription[:200],  # Limit length\n",
        "                'is_hallucination': is_hallucination,\n",
        "                'word_count': word_count,\n",
        "                'inference_time': inference_time\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def analyze_whisperhallu_results(df_results):\n",
        "    \"\"\"\n",
        "    Analyze WhisperHallu effectiveness\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"WHISPERHALLU ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Overall results by prompt type\n",
        "    print(\"\\nHallucination Rates by Prompt Type:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    summary = []\n",
        "\n",
        "    for prompt_type in df_results['prompt_type'].unique():\n",
        "        prompt_data = df_results[df_results['prompt_type'] == prompt_type]\n",
        "        hall_rate = prompt_data['is_hallucination'].mean() * 100\n",
        "        avg_words = prompt_data[prompt_data['is_hallucination']]['word_count'].mean() if any(prompt_data['is_hallucination']) else 0\n",
        "\n",
        "        summary.append({\n",
        "            'Prompt Type': prompt_type,\n",
        "            'Hallucination Rate': f\"{hall_rate:.1f}%\",\n",
        "            'Avg Words': f\"{avg_words:.1f}\",\n",
        "            'Samples': len(prompt_data)\n",
        "        })\n",
        "\n",
        "        print(f\"{prompt_type}:\")\n",
        "        print(f\"  Hallucination rate: {hall_rate:.1f}%\")\n",
        "        print(f\"  Average words when hallucinating: {avg_words:.1f}\")\n",
        "\n",
        "    summary_df = pd.DataFrame(summary)\n",
        "\n",
        "    # Calculate improvement\n",
        "    if 'no_prompt' in df_results['prompt_type'].values:\n",
        "        baseline_rate = df_results[df_results['prompt_type'] == 'no_prompt']['is_hallucination'].mean() * 100\n",
        "\n",
        "        print(\"\\nImprovement over baseline (no prompt):\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        for prompt_type in ['anti_hallucination', 'silence_prompt']:\n",
        "            if prompt_type in df_results['prompt_type'].values:\n",
        "                prompt_rate = df_results[df_results['prompt_type'] == prompt_type]['is_hallucination'].mean() * 100\n",
        "                reduction = baseline_rate - prompt_rate\n",
        "                relative = (reduction / baseline_rate * 100) if baseline_rate > 0 else 0\n",
        "                print(f\"{prompt_type}: {reduction:.1f}% absolute ({relative:.1f}% relative)\")\n",
        "\n",
        "    # Show examples\n",
        "    print(\"\\nExample Hallucinations:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Show baseline hallucinations\n",
        "    baseline_hall = df_results[\n",
        "        (df_results['prompt_type'] == 'no_prompt') &\n",
        "        (df_results['is_hallucination'])\n",
        "    ]\n",
        "\n",
        "    for idx, row in baseline_hall.head(3).iterrows():\n",
        "        print(f\"\\n{row['category']} noise:\")\n",
        "        print(f\"  No prompt: \\\"{row['transcription']}\\\"\")\n",
        "\n",
        "        # Check if prompts prevented it\n",
        "        for prompt_type in ['anti_hallucination', 'silence_prompt']:\n",
        "            prompt_result = df_results[\n",
        "                (df_results['filename'] == row['filename']) &\n",
        "                (df_results['prompt_type'] == prompt_type)\n",
        "            ]\n",
        "\n",
        "            if len(prompt_result) > 0:\n",
        "                if not prompt_result.iloc[0]['is_hallucination']:\n",
        "                    print(f\"  {prompt_type}: (no hallucination) ✓\")\n",
        "                else:\n",
        "                    print(f\"  {prompt_type}: \\\"{prompt_result.iloc[0]['transcription']}\\\"\")\n",
        "\n",
        "    return summary_df\n",
        "\n",
        "def compare_with_standard_whisper(model_size=\"base\", sample_file=None):\n",
        "    \"\"\"\n",
        "    Direct comparison between standard Whisper and WhisperHallu on one file\n",
        "    \"\"\"\n",
        "    if sample_file is None:\n",
        "        sample_file = \"sample_audios/noise_only/human_004.wav\"\n",
        "\n",
        "    print(f\"\\nDirect Comparison on: {sample_file}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Test with standard Whisper\n",
        "    from faster_whisper import WhisperModel\n",
        "\n",
        "    print(\"1. Standard Whisper:\")\n",
        "    standard_model = WhisperModel(model_size)\n",
        "    segments, _ = standard_model.transcribe(sample_file)\n",
        "    standard_text = \" \".join([s.text.strip() for s in segments])\n",
        "    print(f\"   Result: \\\"{standard_text}\\\"\" if standard_text else \"   Result: (no hallucination)\")\n",
        "\n",
        "    # Test with WhisperHallu\n",
        "    print(\"\\n2. WhisperHallu with anti-hallucination prompt:\")\n",
        "    device = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    loadModel(device, modelSize=model_size)\n",
        "\n",
        "    anti_prompt = (\n",
        "        \"Whisper, Ok. \"\n",
        "        \"This is just noise, no speech. \"\n",
        "        \"Ok, Whisper. Whisper, Ok. \"\n",
        "        \"Please do not transcribe anything from noise. \"\n",
        "        \"Ok, Whisper. \"\n",
        "    )\n",
        "\n",
        "    result = transcribePrompt(path=sample_file, lng=\"en\", prompt=anti_prompt)\n",
        "    hallu_text = result.get('text', '').strip() if isinstance(result, dict) else str(result).strip()\n",
        "    print(f\"   Result: \\\"{hallu_text}\\\"\" if hallu_text else \"   Result: (no hallucination)\")\n",
        "\n",
        "    # Analysis\n",
        "    if standard_text and not hallu_text:\n",
        "        print(\"\\n✓ WhisperHallu successfully prevented hallucination!\")\n",
        "    elif not standard_text and not hallu_text:\n",
        "        print(\"\\n✓ Both methods correctly identified no speech\")\n",
        "    elif standard_text and hallu_text:\n",
        "        print(\"\\n⚠ Both methods hallucinated\")\n",
        "    else:\n",
        "        print(\"\\n⚠ WhisperHallu hallucinated while standard Whisper didn't\")\n",
        "\n",
        "def run_whisperhallu_test(model_size=\"base\"):\n",
        "    \"\"\"\n",
        "    Complete WhisperHallu test pipeline\n",
        "    \"\"\"\n",
        "    # Test on noise samples\n",
        "    df_results = test_whisperhallu_on_noise(model_size=model_size, max_samples=10)\n",
        "\n",
        "    # Analyze results\n",
        "    summary = analyze_whisperhallu_results(df_results)\n",
        "\n",
        "    # Save results\n",
        "    output_file = f'whisperhallu_{model_size}_noise_test.csv'\n",
        "    df_results.to_csv(output_file, index=False)\n",
        "    print(f\"\\n✓ Results saved to '{output_file}'\")\n",
        "\n",
        "    # Direct comparison example\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DIRECT COMPARISON EXAMPLE\")\n",
        "    print(\"=\"*60)\n",
        "    compare_with_standard_whisper(model_size=model_size)\n",
        "\n",
        "    return df_results\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Run complete test\n",
        "    results = run_whisperhallu_test(model_size=\"base\")\n",
        "\n",
        "    # Or test single file comparison\n",
        "    # compare_with_standard_whisper(model_size=\"base\", sample_file=\"sample_audios/noise_only/urban_000.wav\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
